{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Exercise three - Low-level functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt \n",
    "from functools import reduce\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 20:\n",
    "Write a simple function that achieves the convolution operation efficiently for twodimensional and three-dimensional inputs. This should allow you to input a set of convolutional filters (‘kernels’ in Keras’s terminology) and an input layer (or image) as inputs. The input layer should have a third dimension, representing a stack of feature maps, and each filter should have a third dimension of corresponding size. The function should output a number of two-dimensional feature maps corresponding to the number of input filters, though these can be stacked into a third dimensional like the input layer. After agreeing on a common function with your group members, show this to your teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    '''A Convolution layer using nxn filters.\n",
    "    \n",
    "    A simple function that achieves the convolution operation efficiently for two-dimensional inputs and three-dimensional inputs. \n",
    "    a set of convolutional filters (‘kernels’ in Keras’s terminology)\n",
    "    an input layer (or image) as inputs. \n",
    "\n",
    "    The input layer should have a third dimension or two dimension, \n",
    "    representing a stack of feature maps, and each filter should have a third dimension of corresponding size. \n",
    "\n",
    "    The function should output a number of two-dimensional feature maps corresponding to the number of input filters, \n",
    "    though these can be stacked into a third dimensional like the input layer. \n",
    "            \n",
    "    TODO: 3d\n",
    "    TODO: padding\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_filters, kernal_size):\n",
    "        '''\n",
    "            filters is a 3 dimensions array (num_filters, 3, 3)\n",
    "        '''\n",
    "        self.num_filters = num_filters\n",
    "        self.kernal_size = kernal_size\n",
    "        self.filters = np.random.randn(num_filters, 3, 3)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def iterate_regions(self, image):\n",
    "        '''Generates image regions    \n",
    "        ''' \n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + self.kernal_size), j:(j + self.kernal_size)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def sub_forward(self, inputs):\n",
    "        '''Return a 3 dimensions array\n",
    "            \n",
    "        ::inputs: 28x28\n",
    "        ::outputs: 26x26x8\n",
    "        '''\n",
    "        # (28, 28)\n",
    "        h, w = inputs.shape\n",
    "\n",
    "        # for now, padding = 0 and stride = 1 \n",
    "        outputs = np.zeros((h - self.kernal_size + 1, w - self.kernal_size + 1, self.num_filters))\n",
    "    \n",
    "        for im_region, i, j in self.iterate_regions(inputs):\n",
    "            outputs[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "        return outputs  \n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if len(inputs.shape) == 2:\n",
    "            return self.sub_forward(inputs)\n",
    "        \n",
    "        elif len(inputs.shape) == 3:\n",
    "            #permuted = np.transpose(inputs, (2, 0, 1))\n",
    "            #c, h, w = permuted.shape\n",
    "            w,h,c = inputs.shape\n",
    "            container = np.zeros((h - self.kernal_size + 1, w - self.kernal_size + 1, self.num_filters))\n",
    "\n",
    "            for i in range(c):\n",
    "                outputs = self.sub_forward(inputs[:,:,i])\n",
    "                container += outputs\n",
    "            return container     \n",
    "        else:\n",
    "            raise AttributeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 21:\n",
    "Write a simple function that achieves rectified linear (relu) activation over a whole feature map, with a threshold at zero. After agreeing on a common function with your group members, show this to your teacher\n",
    "\n",
    "Answer 21:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    '''Activation function Implement\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def relu(self, in_features):\n",
    "        '''A simple function that achieves rectified linear (relu) activation over a whole feature map, with a threshold at zero. \n",
    "\n",
    "        in_features can be numpy array, scalar, vector, or matrix\n",
    "        '''\n",
    "        return np.maximum(0, in_features)\n",
    "\n",
    "    def sigmoid(self, in_features):\n",
    "        '''Apply sigmoid activation function\n",
    "        \n",
    "        in_features can be numpy array, scalar, vector, or matrix\n",
    "        '''\n",
    "        return 1/(1+np.exp(-in_features))\n",
    "    \n",
    "    def leakyRelu(self, in_features, alpha=0.1):\n",
    "        '''Apply leakyRelu activation function\n",
    "        \n",
    "        in_features can be numpy array, scalar, vector, or matrix\n",
    "        '''\n",
    "        return np.where(in_features > 0, in_features, in_features * alpha)      \n",
    "    \n",
    "    def softmax(self, in_features):\n",
    "        '''A function that converts the activation of a 1-dimensional matrix (such as the output of a fully-connected layer) \n",
    "        into a set of probabilities that each matrix element is the most likely classification. \n",
    "\n",
    "        This should include the algorithmic expression of a softmax (normalised exponential) function.\n",
    "        \n",
    "        in_features can be numpy array, scalar, vector, or matrix\n",
    "        '''\n",
    "        expo = np.exp(in_features)\n",
    "        expo_sum = np.sum(expo)\n",
    "        return expo/expo_sum\n",
    "    \n",
    "    def softmax_derive():\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 22:\n",
    "Write a simple function that achieves max pooling. This should allow you to specify the spatial extent of the pooling, with the size of the output feature map changing accordingly. After agreeing on a common function with your group members, show this to your teacher.\n",
    "\n",
    "Answer 22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling:\n",
    "    '''Specify the spatial extent of the pooling, with the size of the output feature map changing accordingly\n",
    "    '''\n",
    "    def __init__(self, pool=2, stride=2):\n",
    "        self.pool = pool \n",
    "        self.stride = stride \n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''Generates non-overlapping kxk image regions to pool over\n",
    "        '''\n",
    "        h, w, c = image.shape\n",
    "        \n",
    "        # floor() the value\n",
    "        new_h = int(np.floor(h/self.pool))\n",
    "        new_w = int(np.floor(w/self.pool))\n",
    "                \n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * self.pool):(i * self.pool + self.stride), (j * self.pool):(j * self.pool + self.stride)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''Apply a forward for the maxpooling layer\n",
    "        \n",
    "        ::output is a 3d numpy array with dimensions (floor(h/2), floor(w/2), num_filters).\n",
    "        ::input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        h, w, num_filters = inputs.shape\n",
    "        \n",
    "        # floor() the value\n",
    "        new_h = int(np.floor(h/self.pool))\n",
    "        new_w = int(np.floor(w/self.pool))\n",
    "        \n",
    "        output = np.zeros((new_h, new_w, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(inputs):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 23:\n",
    "Write a simple function that achieves normalisation within each feature map,\n",
    "modifying the feature map so that its mean value is zero and its standard deviation is one. After agreeing on a common function with your group members, show this to your teacher.\n",
    "\n",
    "Answer 23:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    '''Normalization Implement\n",
    "    \n",
    "    TODO: for all filter or overall?\n",
    "    '''\n",
    "    def __init__(slef):\n",
    "        self.epsilon = np.finfo(float).eps\n",
    "        # self.epsilon=1e-10\n",
    "        \n",
    "    \n",
    "    def zeromean(self,in_features):\n",
    "        '''Normalisation within each feature map, modifying the feature map \n",
    "        so that its mean value is zero and its standard deviation is one.\n",
    "        '''\n",
    "        return (in_features - np.mean(in_features, axis=0))/ ( np.std(in_features, axis=0)+ self.epsilon )\n",
    "    \n",
    "    def minmax(self,in_features):\n",
    "        '''min-max normalization\n",
    "        '''\n",
    "        return (in_features - np.amin(in_features, axis=0)) / (np.amax(in_features, axis=0)-np.amin(in_features, axis=0) + self.epsilon)\n",
    "    \n",
    "    def loge(self,in_features):\n",
    "        '''log transform normalization\n",
    "        \n",
    "        note: np.log is ln, whereas np.log10 is standard base 10 log.\n",
    "        '''\n",
    "        return np.log(in_features)/np.log(np.amax(in_features, axis=0))\n",
    "    \n",
    "    def log10(self,in_features):\n",
    "        '''log transform normalization\n",
    "        \n",
    "        note: np.log is ln, whereas np.log10 is standard base 10 log.\n",
    "        '''\n",
    "        return np.log10(in_features)/np.log10(np.amax(in_features, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 24: \n",
    "Write a function that produces a fully-connected layer. This should allow you to specify the number of output nodes, and link each of these to every node a stack of feature maps. The stack of feature maps will typically be flattened into a 1- dimensional matrix first. After agreeing on a common function with your group members, show this to your teacher. \n",
    "\n",
    "Answer 24:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    '''fully-connected layer\n",
    "    specify the number of output nodes, and link each of these to every node a stack of feature maps. \n",
    "    the stack of feature maps will typically be flattened into a 1-dimensional matrix first. \n",
    "    '''\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        '''Divide by in_dim to reduce the variance of our initial values\n",
    "        \n",
    "        in_dim = Inputs Numbers of Neuron\n",
    "        out_dim = Outputs Numbers of Neuron\n",
    "        '''\n",
    "        self.weights = np.random.randn(in_dim, out_dim) / in_dim\n",
    "        self.biases = np.zeros(out_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''Returns a 1d numpy array\n",
    "        '''\n",
    "        inputs = inputs.flatten()\n",
    "\n",
    "        in_dim, out_dim = self.weights.shape\n",
    "\n",
    "        \n",
    "        outputs = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def clac(in_dim, out_dim=10):\n",
    "        return in_dim.count_dimension(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clac:\n",
    "    '''write some utility funtion \n",
    "    '''\n",
    "    def count_dimension(inputs):\n",
    "        '''count dimention\n",
    "        '''\n",
    "        return reduce(lambda x,y:x*y,inputs.shape)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 25:\n",
    "Write a function that converts the activation of a 1-dimensional matrix (such as the output of a fully-connected layer) into a set of probabilities that each matrix element is the most likely classification. This should include the algorithmic expression of a softmax (normalised exponential) function. After agreeing on a common function with your group members, show this to your teacher.\n",
    "\n",
    "Answer 25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    '''A standard fully-connected layer with softmax activation.\n",
    "    \n",
    "    Refer https://deepai.org/machine-learning-glossary-and-terms/softmax-layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        '''Divide by in_dim to reduce the variance of our initial values\n",
    "        \n",
    "        in_dim = Inputs Numbers of Neuron\n",
    "        out_dim = Outputs Numbers of Neuron\n",
    "        '''\n",
    "\n",
    "        self.weights = np.random.randn(in_dim, out_dim) / in_dim\n",
    "        self.biases = np.zeros(out_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Performs a forward pass of the softmax layer using the given input.\n",
    "        Returns a 1d numpy array containing the respective probability values.\n",
    "        - input can be any array with any dimensions.\n",
    "        '''\n",
    "        inputs = inputs.flatten()\n",
    "\n",
    "        in_dim, out_dim = self.weights.shape\n",
    "\n",
    "        feature = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        # softmax function\n",
    "        expo = np.exp(feature)\n",
    "        expo_sum = np.sum(expo, axis=0)\n",
    "        out = expo / expo_sum\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape (50000, 32, 32, 3)\n",
      "x test shape (10000, 32, 32, 3)\n",
      "y train shape: (50000, 10)\n",
      "y test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255  # (50000, 32, 32, 3)\n",
    "x_test = x_test.astype(\"float32\") / 255 # (10000, 32, 32, 3)\n",
    "\n",
    "# Need an extra dimension for colour channels\n",
    "print(\"x train shape\", x_train.shape)\n",
    "print(\"x test shape\", x_test.shape)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes) # (50000, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes) # (10000, 10)\n",
    "\n",
    "print(\"y train shape:\", y_train.shape)\n",
    "print(\"y test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check test image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "image = x_train[0]\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 8)\n"
     ]
    }
   ],
   "source": [
    "conv = Conv2D(num_filters=8, kernal_size=3)\n",
    "output = conv.forward(image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 8)\n"
     ]
    }
   ],
   "source": [
    "image_reduce = image[:,:,0]\n",
    "output_2d = conv.forward(image_reduce)\n",
    "print(output_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check MaxPooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15, 8)\n"
     ]
    }
   ],
   "source": [
    "maxpool = MaxPooling()\n",
    "output = maxpool.forward(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Second times Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13, 32)\n"
     ]
    }
   ],
   "source": [
    "conv = Conv2D(num_filters=32, kernal_size=3)\n",
    "output = conv.forward(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Second times MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool = MaxPooling()\n",
    "output = maxpool.forward(output)\n",
    "#print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# fully connected layer\n",
    "\n",
    "output = FC(in_dim=clac.count_dimension(output), out_dim=10).forward(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clac.count_dimension(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "output = Softmax(in_dim=clac.count_dimension(output), out_dim=10).forward(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clac.count_dimension(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08359444, 0.12243041, 0.09749358, 0.13145643, 0.09691764,\n",
       "       0.13286025, 0.10252805, 0.07351275, 0.07899229, 0.08021418])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class | It is needed to store the weights of hidden layers and outputs layers\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, hidden, output):\n",
    "        #np.random.seed(10) # for generating the same results\n",
    "        \n",
    "        #np.random.seed(10) # for generating the same results\n",
    "        self.wij   = np.random.rand(hidden.shape[0], hidden.shape[1], hidden.shape[2]) # input to hidden layer weights\n",
    "        self.wjk   = np.random.rand(output.shape[0],1) # hidden layer to output weights\n",
    "        \n",
    "    def sigmoid(self, x, w):\n",
    "        z = np.dot(x, w)\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def conv2d():\n",
    "        pass\n",
    "    \n",
    "    def maxPool():\n",
    "        pass\n",
    "    \n",
    "    def sigmoid_derivative(self, x, w):\n",
    "        return self.sigmoid(x, w) * (1 - self.sigmoid(x, w))\n",
    "    \n",
    "    def gradient_descent(self, x, y, iterations):\n",
    "        for i in range(iterations):\n",
    "            Xi = x\n",
    "            Xj = self.sigmoid(Xi, self.wij)\n",
    "            yhat = self.sigmoid(Xj, self.wjk)\n",
    "            # gradients for hidden to output weights\n",
    "            g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk))\n",
    "            # gradients for input to hidden weights\n",
    "            g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij))\n",
    "            # update weights\n",
    "            self.wij += g_wij\n",
    "            self.wjk += g_wjk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a subset only\n",
    "x = x_train[0]\n",
    "y = y_train[0] #number 6\n",
    "\n",
    "#creating our classes\n",
    "conv = Conv2D(num_filters=32, kernal_size=3)\n",
    "conv_array = conv.forward(x)\n",
    "maxpool = MaxPooling(pool=2, stride=2)\n",
    "maxpool_array = maxpool.forward(conv_array)\n",
    "fc = FC(in_dim=clac.count_dimension(conv_array), out_dim=10)\n",
    "fc_array = fc.forward(conv_array)\n",
    "\n",
    "#create the network\n",
    "nn = NeuralNetwork(x, y.T)\n",
    "\n",
    "#print('Initital weights hidden', nn.wij)\n",
    "#print('\\n Initital weights output', nn.wjk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15, 32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hidden layers shape\n",
    "conv.filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NN Hidden layers shape\n",
    "nn.wij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hidden layers shape\n",
    "conv_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputshape\n",
    "fc_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nn output shape\n",
    "nn.wjk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (32,32,3) and (32,32,3) not aligned: 3 (dim 2) != 32 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-af5a29007e12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-bc006db6ec2d>\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(self, x, y, iterations)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mXi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mXj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwij\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwjk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# gradients for hidden to output weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-bc006db6ec2d>\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(self, x, w)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,32,3) and (32,32,3) not aligned: 3 (dim 2) != 32 (dim 1)"
     ]
    }
   ],
   "source": [
    "#run the model\n",
    "\n",
    "#number of epochs -> how many loops the network will run \n",
    "epochs = 10\n",
    "#learning rate -> how rate the parameters are updating\n",
    "alpha = 0.1 \n",
    "\n",
    "nn.gradient_descent(x, y.T, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#select a subset only\n",
    "select = 80\n",
    "\n",
    "#need to receive softmax\n",
    "def model(inputs, soft=0, first=True):\n",
    "    if first == True:\n",
    "        #create softmax and conv (avoid random weights everytime)        \n",
    "        conv = Conv2D(num_filters=32, kernal_size=3)\n",
    "        output = conv.forward(inputs)\n",
    "        output = MaxPooling(pool=2, stride=2).forward(output)          \n",
    "        output = FC(in_dim=clac.count_dimension(output), out_dim=10).forward(output)\n",
    "        \n",
    "        #need a object softmax to update weights\n",
    "        soft = Softmax(in_dim=clac.count_dimension(output), out_dim=10)                               \n",
    "        output = soft.forward(output)\n",
    "        return soft, output\n",
    "    \n",
    "    else:\n",
    "        output = conv.forward(inputs)\n",
    "        output = MaxPooling(pool=2, stride=2).forward(output)\n",
    "        output = FC(in_dim=clac.count_dimension(output), out_dim=10).forward(output)\n",
    "\n",
    "        #need a object softmax to update weights\n",
    "        output = soft(in_dim=clac.count_dimension(output), out_dim=10).forward(output) \n",
    "        return soft, output\n",
    "    \n",
    "#number of epochs -> how many loops the network will run \n",
    "epochs = 10\n",
    "#learning rate -> how rate the parameters are updating\n",
    "alpha = 0.1 \n",
    "real_values = []\n",
    "predict_values = []\n",
    "\n",
    "for j in range(0,select):\n",
    "    x = x_train[j]\n",
    "    y = y_train[j] #number 6\n",
    "    for i in range(0,epochs):    \n",
    "        if i == 0: #first time :(\n",
    "            #forward phase\n",
    "            soft, output = model(inputs=x)\n",
    "        else:\n",
    "            soft, output = model(inputs=x, soft=soft)\n",
    "       # print(\"output array \\n\", output)\n",
    "       # print('real array', y)\n",
    "       # print('Number predicted', np.argmax(output)) #returns the index of the maximum number\n",
    "\n",
    "        #backward phase \n",
    "        #evaluate error\n",
    "        error = (output - y)\n",
    "        error_cross = -np.log(error)\n",
    "        error_class = np.argmax(output) - np.argmax(y)\n",
    "       # print(\"Error classification\", error_class)\n",
    "       # print('total error', np.sum(error))\n",
    "       #  print(\"\\n\")\n",
    "\n",
    "        #update weights error\n",
    "        # partial derivatives\n",
    "        #need to implement\n",
    "\n",
    "        # average for total gradients (need a function to evaluate eror by position like cross entrophy)\n",
    "\n",
    "        #evalute error by prediction and classification!\n",
    "        if error_class != 0:\n",
    "            #store cross_entrophy error to update weights\n",
    "            total_output = -np.log(error[error_class])\n",
    "            \n",
    "            # update weights of the OBJECT softmax\n",
    "            soft.weights = soft.weights*total_output*-alpha\n",
    "            soft.biases = 0\n",
    "            \n",
    "            #update convulution layer filters as well\n",
    "            #conv.filters = conv.filters*total_output*-alpha\n",
    "\n",
    "            \n",
    "        else:\n",
    "            #stop finding\n",
    "            #print('local optimal')\n",
    "            real_values.append(np.argmax(y))\n",
    "            predict_values.append(np.argmax(output))\n",
    "            break\n",
    "      \n",
    "\n",
    "#final prediction        \n",
    "print(\"output array \\n\", output)\n",
    "print('real array', y)\n",
    "print('Number predicted', np.argmax(output)) #returns the index of the maximum number  \n",
    "print(\"Real Number\", np.argmax(y))\n",
    "print('Cross error', -np.log(error[error_class]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.filters[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Conv2D(num_filters=32, kernal_size=3)\n",
    "output.filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(predict_values)):\n",
    "    print(predict_values[i], real_values[i])\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = soft.weights * 0\n",
    "new = new + 1\n",
    "print(new.shape)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "print(loss.shape)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new * loss.reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_cross = -np.log(error[error_cross])\n",
    "error_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft.weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
